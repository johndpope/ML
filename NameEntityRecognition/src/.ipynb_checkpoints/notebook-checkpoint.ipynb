{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\linhpn.VISC\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "import train\n",
    "import dataset as ds\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "from lstm_crf import EntityLSTM\n",
    "import utils\n",
    "import os\n",
    "import conll2brat\n",
    "import glob\n",
    "import codecs\n",
    "import shutil\n",
    "import time\n",
    "import copy\n",
    "import evaluate\n",
    "import random\n",
    "import pickle\n",
    "import brat2conll\n",
    "import numpy as np\n",
    "import utils_nlp\n",
    "import distutils.util as distutils_util\n",
    "import configparser\n",
    "from pprint import pprint\n",
    "\n",
    "parameters = {'pretrained_model_folder':'../model',\n",
    "                      'dataset_text_folder':'../../../ML_EntityData/data/en',\n",
    "                      'character_embedding_dimension':25,\n",
    "                      'character_lstm_hidden_state_dimension':25,\n",
    "                      'check_for_digits_replaced_with_zeros':True,\n",
    "                      'check_for_lowercase':True,\n",
    "                      'debug':False,\n",
    "                      'dropout_rate':0.5,\n",
    "                      'experiment_name':'test',\n",
    "                      'freeze_token_embeddings':False,\n",
    "                      'gradient_clipping_value':5.0,\n",
    "                      'learning_rate':0.005,\n",
    "                      'load_only_pretrained_token_embeddings':False,\n",
    "                      'load_all_pretrained_token_embeddings':False,\n",
    "                      'main_evaluation_mode':'conll',\n",
    "                      'maximum_number_of_epochs':3,\n",
    "                      'number_of_cpu_threads':8,\n",
    "                      'number_of_gpus':0,\n",
    "                      'optimizer':'sgd',\n",
    "                      'output_folder':'../../../ML_EntityData/output',\n",
    "                      'patience':10,\n",
    "                      'plot_format':'pdf',\n",
    "                      'reload_character_embeddings':True,\n",
    "                      'reload_character_lstm':True,\n",
    "                      'reload_crf':True,\n",
    "                      'reload_feedforward':True,\n",
    "                      'reload_token_embeddings':True,\n",
    "                      'reload_token_lstm':True,\n",
    "                      'remap_unknown_tokens_to_unk':True,\n",
    "                      'spacylanguage':'en',\n",
    "                      'tagging_format':'bioes',\n",
    "                      'token_embedding_dimension':100,\n",
    "                      'token_lstm_hidden_state_dimension':100,\n",
    "                      'token_pretrained_embedding_filepath':'../../../ML_EntityData/embedding/glove.6B.100d.txt',\n",
    "                      'tokenizer':'spacy',\n",
    "                      'train_model':True,\n",
    "                      'use_character_lstm':True,\n",
    "                      'use_crf':True,\n",
    "                      'use_pretrained_model':False,\n",
    "                      'verbose':False}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the validity of BRAT-formatted train set... Done.\n",
      "Checking compatibility between CONLL and BRAT for train_compatible_with_brat set ... Done.\n",
      "Checking validity of CONLL BIOES format... Done.\n",
      "Checking the validity of BRAT-formatted valid set... Done.\n",
      "Checking compatibility between CONLL and BRAT for valid_compatible_with_brat set ... Done.\n",
      "Checking validity of CONLL BIOES format... Done.\n",
      "Checking the validity of BRAT-formatted test set... Done.\n",
      "Checking compatibility between CONLL and BRAT for test_compatible_with_brat set ... Done.\n",
      "Checking validity of CONLL BIOES format... Done.\n",
      "Load dataset... done (43.08 seconds)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load dataset\n",
    "dataset_filepaths, dataset_brat_folders = utils.get_valid_dataset_filepaths(parameters)\n",
    "dataset = ds.Dataset(verbose=False, debug=False)\n",
    "token_to_vector = dataset.load_dataset(dataset_filepaths, parameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\linhpn.VISC\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create model lstm+crf\n",
    "session_conf = tf.ConfigProto(\n",
    "            intra_op_parallelism_threads=parameters['number_of_cpu_threads'],\n",
    "            inter_op_parallelism_threads=parameters['number_of_cpu_threads'],\n",
    "            device_count={'CPU': 1, 'GPU': parameters['number_of_gpus']},\n",
    "            allow_soft_placement=True,\n",
    "            # automatically choose an existing and supported device to run the operations in case the specified one doesn't exist\n",
    "            log_device_placement=False\n",
    "        )\n",
    "sess = tf.Session(config=session_conf)\n",
    "\n",
    "with sess.as_default():\n",
    "    # Create model and initialize or load pretrained model\n",
    "    ### Instantiate the model\n",
    "    model = EntityLSTM(dataset=dataset, token_embedding_dimension=parameters['token_embedding_dimension'],\n",
    "                       character_lstm_hidden_state_dimension=parameters['character_lstm_hidden_state_dimension'],\n",
    "                       token_lstm_hidden_state_dimension=parameters['token_lstm_hidden_state_dimension'],\n",
    "                       character_embedding_dimension=parameters['character_embedding_dimension'],\n",
    "                       use_crf=parameters['use_crf'],\n",
    "                       use_character_lstm=parameters['use_character_lstm'],\n",
    "                       gradient_clipping_value=parameters['gradient_clipping_value'],\n",
    "                       learning_rate=parameters['learning_rate'],\n",
    "                       freeze_token_embeddings=parameters['freeze_token_embeddings'],\n",
    "                       optimizer=parameters['optimizer'],\n",
    "                       maximum_number_of_epochs=parameters['maximum_number_of_epochs'])\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load token embeddings... done (0.38 seconds)\n",
      "number_of_token_original_case_found: 14618\n",
      "number_of_token_lowercase_found: 11723\n",
      "number_of_token_digits_replaced_with_zeros_found: 119\n",
      "number_of_token_lowercase_and_digits_replaced_with_zeros_found: 16\n",
      "number_of_loaded_word_vectors: 26476\n",
      "dataset.vocabulary_size: 28984\n"
     ]
    }
   ],
   "source": [
    "# Load embedding\n",
    "model.load_pretrained_token_embeddings(sess, dataset,embedding_filepath=parameters['token_pretrained_embedding_filepath'],\n",
    "                                                       check_lowercase= parameters['check_for_lowercase'],check_digits=parameters['check_for_digits_replaced_with_zeros'],\n",
    "                                                       token_to_vector=token_to_vector)\n",
    "# Initial params_train\n",
    "transition_params_trained = np.random.rand(len(dataset.unique_labels) + 2,len(dataset.unique_labels) + 2)\n",
    "\n",
    "del token_to_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../model\\model.ckpt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transition_params_trained = model.restore_from_pretrained_model(dataset, sess , model_pathfile=os.path.join(parameters['pretrained_model_folder'],'model.ckpt'),\n",
    "                                                                                     dataset_pathfile=(parameters['pretrained_model_folder']+'/dataset.pickle'),\n",
    "                                                                                     embedding_filepath= parameters['token_pretrained_embedding_filepath'],\n",
    "                                                                                     character_dimension = parameters['character_embedding_dimension'],\n",
    "                                                                                     token_dimension=parameters['token_embedding_dimension'],token_to_vector=token_to_vector)\n",
    "del token_to_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stats_graph_folder, experiment_timestamp = utils.create_stats_graph_folder(parameters)\n",
    "\n",
    "        # Initialize and save execution details\n",
    "start_time = time.time()\n",
    "results = {}\n",
    "results['epoch'] = {}\n",
    "results['execution_details'] = {}\n",
    "results['execution_details']['train_start'] = start_time\n",
    "results['execution_details']['time_stamp'] = experiment_timestamp\n",
    "results['execution_details']['early_stop'] = False\n",
    "results['execution_details']['keyboard_interrupt'] = False\n",
    "results['execution_details']['num_epochs'] = 0\n",
    "results['model_options'] = copy.copy(parameters)\n",
    "\n",
    "model_folder = os.path.join(stats_graph_folder, 'model')\n",
    "utils.create_folder_if_not_exists(model_folder)\n",
    "\n",
    "pickle.dump(dataset, open(os.path.join(model_folder, 'dataset.pickle'), 'wb'))\n",
    "\n",
    "# tensorboard_log_folder = os.path.join(stats_graph_folder, 'tensorboard_logs')\n",
    "# utils.create_folder_if_not_exists(tensorboard_log_folder)\n",
    "# tensorboard_log_folders = {}\n",
    "# for dataset_type in dataset_filepaths.keys():\n",
    "#     tensorboard_log_folders[dataset_type] = os.path.join(stats_graph_folder, 'tensorboard_logs', dataset_type)\n",
    "#     utils.create_folder_if_not_exists(tensorboard_log_folders[dataset_type])\n",
    "\n",
    "# Instantiate the writers for TensorBoard\n",
    "# writers = {}\n",
    "# for dataset_type in dataset_filepaths.keys():\n",
    "#     writers[dataset_type] = tf.summary.FileWriter(tensorboard_log_folders[dataset_type], graph=sess.graph)\n",
    "# embedding_writer = tf.summary.FileWriter(\n",
    "#     model_folder)  # embedding_writer has to write in model_folder, otherwise TensorBoard won't be able to view embeddings\n",
    "\n",
    "# embeddings_projector_config = projector.ProjectorConfig()\n",
    "# tensorboard_token_embeddings = embeddings_projector_config.embeddings.add()\n",
    "# tensorboard_token_embeddings.tensor_name = model.token_embedding_weights.name\n",
    "# token_list_file_path = os.path.join(model_folder, 'tensorboard_metadata_tokens.tsv')\n",
    "# tensorboard_token_embeddings.metadata_path = os.path.relpath(token_list_file_path, '..')\n",
    "\n",
    "# tensorboard_character_embeddings = embeddings_projector_config.embeddings.add()\n",
    "# tensorboard_character_embeddings.tensor_name = model.character_embedding_weights.name\n",
    "# character_list_file_path = os.path.join(model_folder, 'tensorboard_metadata_characters.tsv')\n",
    "# tensorboard_character_embeddings.metadata_path = os.path.relpath(character_list_file_path, '..')\n",
    "\n",
    "# projector.visualize_embeddings(embedding_writer, embeddings_projector_config)\n",
    "\n",
    "# Write metadata for TensorBoard embeddings\n",
    "# token_list_file = codecs.open(token_list_file_path, 'w', 'UTF-8')\n",
    "# for token_index in range(dataset.vocabulary_size):\n",
    "#     token_list_file.write('{0}\\n'.format(dataset.index_to_token[token_index]))\n",
    "# token_list_file.close()\n",
    "\n",
    "# character_list_file = codecs.open(character_list_file_path, 'w', 'UTF-8')\n",
    "# for character_index in range(dataset.alphabet_size):\n",
    "#     if character_index == dataset.PADDING_CHARACTER_INDEX:\n",
    "#         character_list_file.write('PADDING\\n')\n",
    "#     else:\n",
    "#         character_list_file.write('{0}\\n'.format(dataset.index_to_character[character_index]))\n",
    "# character_list_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting epoch 0\n",
      "Training completed in 0.00 seconds\n",
      "Evaluate model on the train set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC     0.9335    0.8905    0.9115      1041\n",
      "     B-MISC     0.9122    0.7751    0.8381       858\n",
      "      B-ORG     0.9359    0.8757    0.9048      2485\n",
      "      B-PER     0.9466    0.9897    0.9677      4284\n",
      "      E-LOC     0.9353    0.8886    0.9113      1041\n",
      "     E-MISC     0.9261    0.7890    0.8521       858\n",
      "      E-ORG     0.9401    0.8777    0.9078      2485\n",
      "      E-PER     0.9517    0.9939    0.9724      4284\n",
      "      I-LOC     0.8049    0.5690    0.6667       116\n",
      "     I-MISC     0.8829    0.6599    0.7553       297\n",
      "      I-ORG     0.7845    0.9286    0.8505      1219\n",
      "      I-PER     0.8866    0.8975    0.8921       244\n",
      "          O     0.0000    0.0000    0.0000       490\n",
      "      S-LOC     0.9651    0.9647    0.9649      6099\n",
      "     S-MISC     0.9188    0.9039    0.9113      2580\n",
      "      S-ORG     0.9553    0.8918    0.9225      3836\n",
      "      S-PER     0.8604    0.9581    0.9066      2316\n",
      "\n",
      "avg / total     0.9197    0.9127    0.9149     34533\n",
      "\n",
      "Evaluate model on the valid set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC     0.8969    0.8547    0.8753       234\n",
      "     B-MISC     0.8419    0.7043    0.7669       257\n",
      "      B-ORG     0.8482    0.7822    0.8139       450\n",
      "      B-PER     0.9650    0.9838    0.9743      1234\n",
      "      E-LOC     0.8924    0.8504    0.8709       234\n",
      "     E-MISC     0.8716    0.7393    0.8000       257\n",
      "      E-ORG     0.8671    0.7978    0.8310       450\n",
      "      E-PER     0.9666    0.9838    0.9751      1234\n",
      "      I-LOC     0.7895    0.6522    0.7143        23\n",
      "     I-MISC     0.7460    0.5281    0.6184        89\n",
      "      I-ORG     0.7330    0.8571    0.7902       301\n",
      "      I-PER     0.8772    0.6849    0.7692        73\n",
      "          O     0.0000    0.0000    0.0000       154\n",
      "      S-LOC     0.9593    0.9707    0.9650      1603\n",
      "     S-MISC     0.9059    0.8977    0.9018       665\n",
      "      S-ORG     0.9357    0.8822    0.9081       891\n",
      "      S-PER     0.8169    0.9539    0.8801       608\n",
      "\n",
      "avg / total     0.8969    0.8905    0.8922      8757\n",
      "\n",
      "Evaluate model on the test set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC     0.7814    0.8319    0.8058       232\n",
      "     B-MISC     0.6369    0.6441    0.6404       177\n",
      "      B-ORG     0.8351    0.8048    0.8197       579\n",
      "      B-PER     0.9634    0.9936    0.9782      1086\n",
      "      E-LOC     0.7680    0.8276    0.7967       232\n",
      "     E-MISC     0.6760    0.6836    0.6798       177\n",
      "      E-ORG     0.8441    0.8135    0.8285       579\n",
      "      E-PER     0.9616    0.9908    0.9760      1086\n",
      "      I-LOC     0.4545    0.4000    0.4255        25\n",
      "     I-MISC     0.5854    0.6154    0.6000        39\n",
      "      I-ORG     0.6257    0.8945    0.7363       256\n",
      "      I-PER     0.9306    0.9571    0.9437        70\n",
      "          O     0.0000    0.0000    0.0000       346\n",
      "      S-LOC     0.9272    0.9318    0.9295      1436\n",
      "     S-MISC     0.8349    0.8476    0.8412       525\n",
      "      S-ORG     0.9150    0.8558    0.8844      1082\n",
      "      S-PER     0.8453    0.9058    0.8745       531\n",
      "\n",
      "avg / total     0.8421    0.8550    0.8475      8458\n",
      "\n",
      "\n",
      "Starting epoch 1\n",
      "Training completed in 306.77 seconds\n",
      "Evaluate model on the train set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC     0.9137    0.9260    0.9198      1041\n",
      "     B-MISC     0.8995    0.8660    0.8824       858\n",
      "      B-ORG     0.9312    0.9207    0.9259      2485\n",
      "      B-PER     0.9796    0.9874    0.9835      4284\n",
      "      E-LOC     0.9118    0.9241    0.9179      1041\n",
      "     E-MISC     0.8993    0.8636    0.8811       858\n",
      "      E-ORG     0.9324    0.9219    0.9272      2485\n",
      "      E-PER     0.9835    0.9909    0.9872      4284\n",
      "      I-LOC     0.8452    0.6121    0.7100       116\n",
      "     I-MISC     0.8861    0.7071    0.7865       297\n",
      "      I-ORG     0.9124    0.8712    0.8913      1219\n",
      "      I-PER     0.9528    0.9098    0.9308       244\n",
      "          O     0.0000    0.0000    0.0000       309\n",
      "      S-LOC     0.9728    0.9662    0.9695      6099\n",
      "     S-MISC     0.9321    0.9105    0.9212      2580\n",
      "      S-ORG     0.9615    0.9176    0.9390      3836\n",
      "      S-PER     0.9341    0.9547    0.9443      2316\n",
      "\n",
      "avg / total     0.9426    0.9316    0.9368     34352\n",
      "\n",
      "Evaluate model on the valid set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC     0.9095    0.9017    0.9056       234\n",
      "     B-MISC     0.8216    0.7704    0.7952       257\n",
      "      B-ORG     0.8421    0.8178    0.8298       450\n",
      "      B-PER     0.9854    0.9862    0.9858      1234\n",
      "      E-LOC     0.9004    0.8889    0.8946       234\n",
      "     E-MISC     0.8525    0.8093    0.8303       257\n",
      "      E-ORG     0.8627    0.8378    0.8501       450\n",
      "      E-PER     0.9838    0.9838    0.9838      1234\n",
      "      I-LOC     0.8000    0.5217    0.6316        23\n",
      "     I-MISC     0.7344    0.5281    0.6144        89\n",
      "      I-ORG     0.8973    0.7841    0.8369       301\n",
      "      I-PER     0.9630    0.7123    0.8189        73\n",
      "          O     0.0000    0.0000    0.0000       106\n",
      "      S-LOC     0.9577    0.9738    0.9657      1603\n",
      "     S-MISC     0.9183    0.8962    0.9072       665\n",
      "      S-ORG     0.9320    0.9080    0.9198       891\n",
      "      S-PER     0.8941    0.9441    0.9184       608\n",
      "\n",
      "avg / total     0.9180    0.9057    0.9112      8709\n",
      "\n",
      "Evaluate model on the test set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC     0.7292    0.8707    0.7937       232\n",
      "     B-MISC     0.6237    0.6836    0.6523       177\n",
      "      B-ORG     0.8500    0.8221    0.8358       579\n",
      "      B-PER     0.9745    0.9862    0.9803      1086\n",
      "      E-LOC     0.7283    0.8664    0.7913       232\n",
      "     E-MISC     0.6443    0.7062    0.6739       177\n",
      "      E-ORG     0.8623    0.8325    0.8471       579\n",
      "      E-PER     0.9745    0.9834    0.9789      1086\n",
      "      I-LOC     0.5417    0.5200    0.5306        25\n",
      "     I-MISC     0.6774    0.5385    0.6000        39\n",
      "      I-ORG     0.8015    0.8516    0.8258       256\n",
      "      I-PER     0.9286    0.9286    0.9286        70\n",
      "          O     0.0000    0.0000    0.0000       276\n",
      "      S-LOC     0.9330    0.9304    0.9317      1436\n",
      "     S-MISC     0.8200    0.8419    0.8308       525\n",
      "      S-ORG     0.9091    0.8688    0.8885      1082\n",
      "      S-PER     0.9006    0.8701    0.8851       531\n",
      "\n",
      "avg / total     0.8599    0.8635    0.8611      8388\n",
      "\n",
      "\n",
      "Starting epoch 2\n",
      "Training completed in 295.98 seconds\n",
      "Evaluate model on the train set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC     0.9498    0.9087    0.9288      1041\n",
      "     B-MISC     0.9222    0.8287    0.8729       858\n",
      "      B-ORG     0.9193    0.9215    0.9204      2485\n",
      "      B-PER     0.9774    0.9897    0.9835      4284\n",
      "      E-LOC     0.9469    0.9078    0.9269      1041\n",
      "     E-MISC     0.9248    0.8310    0.8754       858\n",
      "      E-ORG     0.9213    0.9235    0.9224      2485\n",
      "      E-PER     0.9809    0.9930    0.9869      4284\n",
      "      I-LOC     0.9178    0.5776    0.7090       116\n",
      "     I-MISC     0.9095    0.6431    0.7535       297\n",
      "      I-ORG     0.9081    0.8917    0.8998      1219\n",
      "      I-PER     0.9567    0.9057    0.9305       244\n",
      "          O     0.0000    0.0000    0.0000       249\n",
      "      S-LOC     0.9706    0.9705    0.9706      6099\n",
      "     S-MISC     0.9689    0.8950    0.9305      2580\n",
      "      S-ORG     0.9512    0.9301    0.9406      3836\n",
      "      S-PER     0.9579    0.9439    0.9508      2316\n",
      "\n",
      "avg / total     0.9485    0.9315    0.9394     34292\n",
      "\n",
      "Evaluate model on the valid set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC     0.9498    0.8889    0.9183       234\n",
      "     B-MISC     0.8267    0.7237    0.7718       257\n",
      "      B-ORG     0.8628    0.8244    0.8432       450\n",
      "      B-PER     0.9831    0.9895    0.9863      1234\n",
      "      E-LOC     0.9358    0.8718    0.9027       234\n",
      "     E-MISC     0.8634    0.7626    0.8099       257\n",
      "      E-ORG     0.8747    0.8378    0.8558       450\n",
      "      E-PER     0.9815    0.9870    0.9842      1234\n",
      "      I-LOC     0.8571    0.5217    0.6486        23\n",
      "     I-MISC     0.7627    0.5056    0.6081        89\n",
      "      I-ORG     0.9125    0.7973    0.8511       301\n",
      "      I-PER     0.9630    0.7123    0.8189        73\n",
      "          O     0.0000    0.0000    0.0000        63\n",
      "      S-LOC     0.9565    0.9732    0.9647      1603\n",
      "     S-MISC     0.9513    0.8812    0.9149       665\n",
      "      S-ORG     0.9285    0.9181    0.9233       891\n",
      "      S-PER     0.9153    0.9243    0.9198       608\n",
      "\n",
      "avg / total     0.9305    0.9065    0.9174      8666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluate model on the test set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC     0.7795    0.8534    0.8148       232\n",
      "     B-MISC     0.6188    0.6328    0.6257       177\n",
      "      B-ORG     0.8453    0.8117    0.8282       579\n",
      "      B-PER     0.9729    0.9899    0.9813      1086\n",
      "      E-LOC     0.7747    0.8448    0.8082       232\n",
      "     E-MISC     0.6464    0.6610    0.6536       177\n",
      "      E-ORG     0.8559    0.8204    0.8377       579\n",
      "      E-PER     0.9737    0.9880    0.9808      1086\n",
      "      I-LOC     0.5500    0.4400    0.4889        25\n",
      "     I-MISC     0.6154    0.4103    0.4923        39\n",
      "      I-ORG     0.7753    0.8086    0.7916       256\n",
      "      I-PER     0.9286    0.9286    0.9286        70\n",
      "          O     0.0000    0.0000    0.0000       235\n",
      "      S-LOC     0.9278    0.9311    0.9294      1436\n",
      "     S-MISC     0.8659    0.8114    0.8378       525\n",
      "      S-ORG     0.8941    0.8817    0.8879      1082\n",
      "      S-PER     0.9457    0.8531    0.8970       531\n",
      "\n",
      "avg / total     0.8675    0.8608    0.8637      8347\n",
      "\n",
      "\n",
      "Starting epoch 3\n",
      "Training completed in 304.69 seconds\n",
      "Evaluate model on the train set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC     0.9424    0.9270    0.9346      1041\n",
      "     B-MISC     0.9071    0.8881    0.8975       858\n",
      "      B-ORG     0.9463    0.9356    0.9409      2485\n",
      "      B-PER     0.9794    0.9900    0.9847      4284\n",
      "      E-LOC     0.9388    0.9280    0.9333      1041\n",
      "     E-MISC     0.9037    0.8858    0.8946       858\n",
      "      E-ORG     0.9450    0.9340    0.9395      2485\n",
      "      E-PER     0.9815    0.9921    0.9868      4284\n",
      "      I-LOC     0.8283    0.7069    0.7628       116\n",
      "     I-MISC     0.9180    0.7542    0.8281       297\n",
      "      I-ORG     0.9461    0.8778    0.9106      1219\n",
      "      I-PER     0.9544    0.9426    0.9485       244\n",
      "          O     0.0000    0.0000    0.0000       352\n",
      "      S-LOC     0.9561    0.9826    0.9692      6099\n",
      "     S-MISC     0.9278    0.9368    0.9323      2580\n",
      "      S-ORG     0.9685    0.9291    0.9484      3836\n",
      "      S-PER     0.9547    0.9551    0.9549      2316\n",
      "\n",
      "avg / total     0.9455    0.9415    0.9432     34395\n",
      "\n",
      "Evaluate model on the valid set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC     0.9327    0.8889    0.9103       234\n",
      "     B-MISC     0.7816    0.7938    0.7876       257\n",
      "      B-ORG     0.8548    0.8244    0.8394       450\n",
      "      B-PER     0.9854    0.9870    0.9862      1234\n",
      "      E-LOC     0.9279    0.8803    0.9035       234\n",
      "     E-MISC     0.8106    0.8327    0.8215       257\n",
      "      E-ORG     0.8670    0.8400    0.8533       450\n",
      "      E-PER     0.9846    0.9854    0.9850      1234\n",
      "      I-LOC     0.8750    0.6087    0.7179        23\n",
      "     I-MISC     0.6761    0.5393    0.6000        89\n",
      "      I-ORG     0.9030    0.8040    0.8506       301\n",
      "      I-PER     0.9623    0.6986    0.8095        73\n",
      "          O     0.0000    0.0000    0.0000       116\n",
      "      S-LOC     0.9492    0.9788    0.9638      1603\n",
      "     S-MISC     0.9028    0.9083    0.9055       665\n",
      "      S-ORG     0.9481    0.9226    0.9352       891\n",
      "      S-PER     0.9140    0.9260    0.9199       608\n",
      "\n",
      "avg / total     0.9170    0.9093    0.9126      8719\n",
      "\n",
      "Evaluate model on the test set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC     0.7795    0.8534    0.8148       232\n",
      "     B-MISC     0.6100    0.6893    0.6472       177\n",
      "      B-ORG     0.8325    0.8411    0.8368       579\n",
      "      B-PER     0.9764    0.9917    0.9840      1086\n",
      "      E-LOC     0.7817    0.8491    0.8140       232\n",
      "     E-MISC     0.6350    0.7175    0.6737       177\n",
      "      E-ORG     0.8425    0.8497    0.8461       579\n",
      "      E-PER     0.9764    0.9890    0.9826      1086\n",
      "      I-LOC     0.5238    0.4400    0.4783        25\n",
      "     I-MISC     0.5714    0.6154    0.5926        39\n",
      "      I-ORG     0.7889    0.8320    0.8099       256\n",
      "      I-PER     0.9437    0.9571    0.9504        70\n",
      "          O     0.0000    0.0000    0.0000       326\n",
      "      S-LOC     0.9112    0.9366    0.9238      1436\n",
      "     S-MISC     0.8040    0.8362    0.8198       525\n",
      "      S-ORG     0.9100    0.8872    0.8985      1082\n",
      "      S-PER     0.9423    0.8606    0.8996       531\n",
      "\n",
      "avg / total     0.8523    0.8639    0.8577      8438\n",
      "\n",
      "Finishing the experiment\n",
      "5705.929584264755\n"
     ]
    }
   ],
   "source": [
    "bad_counter = 0  # number of epochs with no improvement on the validation test in terms of F1-score\n",
    "previous_best_valid_f1_score = 0\n",
    "epoch_number = -1\n",
    "try:\n",
    "    while True:\n",
    "\n",
    "        step = 0\n",
    "        epoch_number += 1\n",
    "        print('\\nStarting epoch {0}'.format(epoch_number))\n",
    "\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        if epoch_number != 0:\n",
    "            # Train model: loop over all sequences of training set with shuffling\n",
    "            sequence_numbers = list(range(len(dataset.token_indices['train'])))\n",
    "            random.shuffle(sequence_numbers)\n",
    "            for sequence_number in sequence_numbers:\n",
    "                transition_params_trained = train.train_step(sess, dataset, sequence_number, model, parameters['dropout_rate'])\n",
    "                step += 1\n",
    "                if step % 10 == 0:\n",
    "                    print('Training {0:.2f}% done'.format(step / len(sequence_numbers) * 100), end='\\r', flush=True)\n",
    "\n",
    "        epoch_elapsed_training_time = time.time() - epoch_start_time\n",
    "        print('Training completed in {0:.2f} seconds'.format(epoch_elapsed_training_time), flush=True)\n",
    "\n",
    "        y_pred, y_true, output_filepaths = train.predict_labels_lite(sess=sess,model= model,transition_params_trained= transition_params_trained,\n",
    "                                                                         dataset=dataset,epoch_number= epoch_number,\n",
    "                                                                        stats_graph_folder= stats_graph_folder,dataset_filepaths= dataset_filepaths,\n",
    "                                                                        tagging_format= parameters['tagging_format'], main_evaluation_mode=parameters['main_evaluation_mode'],use_crf=parameters['use_crf'])\n",
    "\n",
    "        # # Evaluate model: save and plot results\n",
    "        # evaluate.evaluate_model(results, dataset, y_pred, y_true, stats_graph_folder, epoch_number,\n",
    "        #                                 epoch_start_time, output_filepaths, parameters)\n",
    "        #\n",
    "        # if parameters['use_pretrained_model'] and not parameters['train_model']:\n",
    "        #     conll2brat.output_brat(output_filepaths, dataset_brat_folders, stats_graph_folder)\n",
    "        #     break\n",
    "        #\n",
    "        # # Save model\n",
    "        model.saver.save(sess, os.path.join(model_folder, 'model_{0:05d}.ckpt'.format(epoch_number)))\n",
    "        #\n",
    "        # # Save TensorBoard logs\n",
    "        # summary = sess.run(model.summary_op, feed_dict=None)\n",
    "        # writers['train'].add_summary(summary, epoch_number)\n",
    "        # writers['train'].flush()\n",
    "        # utils.copytree(writers['train'].get_logdir(), model_folder)\n",
    "        #\n",
    "        # # Early stop\n",
    "        # valid_f1_score = results['epoch'][epoch_number][0]['valid']['f1_score']['micro']\n",
    "        # if valid_f1_score > previous_best_valid_f1_score:\n",
    "        #     bad_counter = 0\n",
    "        #     previous_best_valid_f1_score = valid_f1_score\n",
    "        #     conll2brat.output_brat(output_filepaths, dataset_brat_folders, stats_graph_folder,\n",
    "        #                                       overwrite=True)\n",
    "        #     transition_params_trained = transition_params_trained\n",
    "        # else:\n",
    "        #     bad_counter += 1\n",
    "        # print(\"The last {0} epochs have not shown improvements on the validation set.\".format(bad_counter))\n",
    "        #\n",
    "        # if bad_counter >= parameters['patience']:\n",
    "        #     print('Early Stop!')\n",
    "        #     results['execution_details']['early_stop'] = True\n",
    "        #     break\n",
    "\n",
    "        if epoch_number >= parameters['maximum_number_of_epochs']: break\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    results['execution_details']['keyboard_interrupt'] = True\n",
    "    print('Training interrupted')\n",
    "\n",
    "print('Finishing the experiment')\n",
    "end_time = time.time()\n",
    "print(end_time-start_time)\n",
    "# results['execution_details']['train_duration'] = end_time - start_time\n",
    "# results['execution_details']['train_end'] = end_time\n",
    "# evaluate.save_results(results, stats_graph_folder)\n",
    "# for dataset_type in dataset_filepaths.keys():\n",
    "#     writers[dataset_type].close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction_count=0\n",
    "\n",
    "\n",
    "def predict(text):\n",
    "    #         if prediction_count == 1:\n",
    "    parameters['dataset_text_folder'] = os.path.join('..', 'data', 'temp')\n",
    "    stats_graph_folder, _ = utils.create_stats_graph_folder(parameters)\n",
    "\n",
    "    # Update the deploy folder, file, and dataset\n",
    "    dataset_type = 'deploy'\n",
    "    ### Delete all deployment data\n",
    "    for filepath in glob.glob(os.path.join(parameters['dataset_text_folder'], '{0}*'.format(dataset_type))):\n",
    "        if os.path.isdir(filepath):\n",
    "            shutil.rmtree(filepath)\n",
    "        else:\n",
    "            os.remove(filepath)\n",
    "    ### Create brat folder and file\n",
    "    dataset_brat_deploy_folder = os.path.join(parameters['dataset_text_folder'], dataset_type)\n",
    "    utils.create_folder_if_not_exists(dataset_brat_deploy_folder)\n",
    "    dataset_brat_deploy_filepath = os.path.join(dataset_brat_deploy_folder, 'temp_{0}.txt'.format(\n",
    "        str(prediction_count).zfill(5)))  # self._get_dataset_brat_deploy_filepath(dataset_brat_deploy_folder)\n",
    "    with codecs.open(dataset_brat_deploy_filepath, 'w', 'UTF-8') as f:\n",
    "        f.write(text)\n",
    "    ### Update deploy filepaths\n",
    "    dataset_filepaths, dataset_brat_folders = utils.get_valid_dataset_filepaths(parameters,\n",
    "                                                                           dataset_types=[dataset_type])\n",
    "    dataset_filepaths.update(dataset_filepaths)\n",
    "    dataset_brat_folders.update(dataset_brat_folders)\n",
    "    ### Update the dataset for the new deploy set\n",
    "    dataset.update_dataset(dataset_filepaths, [dataset_type])\n",
    "\n",
    "    # Predict labels and output brat\n",
    "    output_filepaths = {}\n",
    "    prediction_output = train.prediction_step(sess, dataset, dataset_type, model,\n",
    "                                              transition_params_trained, stats_graph_folder,\n",
    "                                              prediction_count, dataset_filepaths, parameters['tagging_format'],\n",
    "                                              parameters['main_evaluation_mode'])\n",
    "    _, _, output_filepaths[dataset_type] = prediction_output\n",
    "    conll2brat.output_brat(output_filepaths, dataset_brat_folders, stats_graph_folder, overwrite=True)\n",
    "\n",
    "    # Print and output result\n",
    "    text_filepath = os.path.join(stats_graph_folder, 'brat', 'deploy',\n",
    "                                 os.path.basename(dataset_brat_deploy_filepath))\n",
    "    annotation_filepath = os.path.join(stats_graph_folder, 'brat', 'deploy', '{0}.ann'.format(\n",
    "        utils.get_basename_without_extension(dataset_brat_deploy_filepath)))\n",
    "    text2, entities = brat2conll.get_entities_from_brat(text_filepath, annotation_filepath, verbose=True)\n",
    "    assert (text == text2)\n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting deploy set from BRAT to CONLL... Done.\n",
      "Converting CONLL from BIO to BIOES format... Done.\n",
      "Predict labels for the deploy set\n",
      "Formatting 000_deploy set from CONLL to BRAT... Done.\n",
      "\n",
      "text:\n",
      "When John Hennessy announced in 2015 that he was stepping down as president of Stanford University the following year, he said, \"The time has come to return to what brought me to Stanford—teaching and research.\"\n",
      "\n",
      "entity: {'id': 'T1', 'type': 'PER', 'start': 5, 'end': 18, 'text': 'John Hennessy'}\n",
      "entity: {'id': 'T2', 'type': 'ORG', 'start': 79, 'end': 98, 'text': 'Stanford University'}\n",
      "entity: {'id': 'T3', 'type': 'MISC', 'start': 179, 'end': 187, 'text': 'Stanford'}\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'end': 18, 'id': 'T1', 'start': 5, 'text': 'John Hennessy', 'type': 'PER'},\n",
       " {'end': 98,\n",
       "  'id': 'T2',\n",
       "  'start': 79,\n",
       "  'text': 'Stanford University',\n",
       "  'type': 'ORG'},\n",
       " {'end': 187, 'id': 'T3', 'start': 179, 'text': 'Stanford', 'type': 'MISC'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"When John Hennessy announced in 2015 that he was stepping down as president of Stanford University the following year, he said, \\\"The time has come to return to what brought me to Stanford—teaching and research.\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting deploy set from BRAT to CONLL... Done.\n",
      "Converting CONLL from BIO to BIOES format... Done.\n",
      "Predict labels for the deploy set\n",
      "Formatting 000_deploy set from CONLL to BRAT... Done.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../../ML_EntityData/output\\\\temp_2018-02-02_12-06-57-764858\\\\brat\\\\deploy\\\\temp_00000.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-8173d13f7e9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-d050b2e6cac4>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     44\u001b[0m     annotation_filepath = os.path.join(stats_graph_folder, 'brat', 'deploy', '{0}.ann'.format(\n\u001b[0;32m     45\u001b[0m         utils.get_basename_without_extension(dataset_brat_deploy_filepath)))\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[0mtext2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbrat2conll\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_entities_from_brat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_filepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannotation_filepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtext2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mentities\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\linhpn.VISC\\Documents\\ML\\NameEntityRecognition\\src\\brat2conll.py\u001b[0m in \u001b[0;36mget_entities_from_brat\u001b[1;34m(text_filepath, annotation_filepath, verbose)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_entities_from_brat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_filepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannotation_filepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;31m# load text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_filepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'UTF-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\ntext:\\n{0}\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\linhpn.VISC\\AppData\\Local\\Continuum\\Anaconda3\\lib\\codecs.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(filename, mode, encoding, errors, buffering)\u001b[0m\n\u001b[0;32m    893\u001b[0m         \u001b[1;31m# Force opening of the file in binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m         \u001b[0mmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'b'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m     \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../ML_EntityData/output\\\\temp_2018-02-02_12-06-57-764858\\\\brat\\\\deploy\\\\temp_00000.txt'"
     ]
    }
   ],
   "source": [
    "predict('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
