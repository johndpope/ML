{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\linhpn.VISC\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "def BLSTM(input, hidden_state_dimension, initializer, sequence_length=None, output_sequence=True):\n",
    "    with tf.variable_scope(\"bidirectional_LSTM\"):\n",
    "        if sequence_length == None:\n",
    "            batch_size = 1\n",
    "            sequence_length = tf.shape(input)[1]\n",
    "            sequence_length = tf.expand_dims(sequence_length, axis=0, name='sequence_length')\n",
    "        else:\n",
    "            batch_size = tf.shape(sequence_length)[0]\n",
    "\n",
    "        lstm_cell = {}\n",
    "        initial_state = {}\n",
    "        for direction in [\"forward\", \"backward\"]:\n",
    "            with tf.variable_scope(direction):\n",
    "                # LSTM cell\n",
    "                lstm_cell[direction] = tf.contrib.rnn.CoupledInputForgetGateLSTMCell(hidden_state_dimension,\n",
    "                                                                                     forget_bias=1.0,\n",
    "                                                                                     initializer=initializer,\n",
    "                                                                                     state_is_tuple=True)\n",
    "                # initial state: http://stackoverflow.com/questions/38441589/tensorflow-rnn-initial-state\n",
    "                initial_cell_state = tf.get_variable(\"initial_cell_state\", shape=[1, hidden_state_dimension],\n",
    "                                                     dtype=tf.float32, initializer=initializer)\n",
    "                initial_output_state = tf.get_variable(\"initial_output_state\", shape=[1, hidden_state_dimension],\n",
    "                                                       dtype=tf.float32, initializer=initializer)\n",
    "                c_states = tf.tile(initial_cell_state, tf.stack([batch_size, 1]))\n",
    "                h_states = tf.tile(initial_output_state, tf.stack([batch_size, 1]))\n",
    "                initial_state[direction] = tf.contrib.rnn.LSTMStateTuple(c_states, h_states)\n",
    "\n",
    "        # sequence_length must be provided for tf.nn.bidirectional_dynamic_rnn due to internal bug\n",
    "        outputs, final_states = tf.nn.bidirectional_dynamic_rnn(lstm_cell[\"forward\"],\n",
    "                                                                lstm_cell[\"backward\"],\n",
    "                                                                input,\n",
    "                                                                dtype=tf.float32,\n",
    "                                                                sequence_length=sequence_length,\n",
    "                                                                initial_state_fw=initial_state[\"forward\"],\n",
    "                                                                initial_state_bw=initial_state[\"backward\"])\n",
    "        if output_sequence == True:\n",
    "            outputs_forward, outputs_backward = outputs\n",
    "            output = tf.concat([outputs_forward, outputs_backward], axis=2, name='output_sequence')\n",
    "        else:\n",
    "            # max pooling\n",
    "            #             outputs_forward, outputs_backward = outputs\n",
    "            #             output = tf.concat([outputs_forward, outputs_backward], axis=2, name='output_sequence')\n",
    "            #             output = tf.reduce_max(output, axis=1, name='output')\n",
    "            # last pooling\n",
    "            final_states_forward, final_states_backward = final_states\n",
    "            output = tf.concat([final_states_forward[1], final_states_backward[1]], axis=1, name='output')\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Char_BLSTM_CRF(object):\n",
    "    \"\"\"\n",
    "    An LSTM architecture for named entity recognition.\n",
    "    Uses a character embedding layer followed by an LSTM to generate vector representation from characters for each token.\n",
    "    Then the character vector is concatenated with token embedding vector, which is input to another LSTM  followed by a CRF layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset,token_embedding_dimension,character_lstm_hidden_state_dimension,\n",
    "                 token_lstm_hidden_state_dimension,character_embedding_dimension,\n",
    "                 freeze_token_embeddings=False,\n",
    "                 learning_rate=0.005, gradient_clipping_value=5.0, optimizer='sgd',maximum_number_of_epochs=30):\n",
    "      \n",
    "        self.verbose = True\n",
    "        self.input_token_indices = tf.placeholder(tf.int32, [None], name=\"input_token_indices\")\n",
    "        self.input_label_indices_vector = tf.placeholder(tf.float32, [None, dataset.number_of_classes],\n",
    "                                                         name=\"input_label_indices_vector\")\n",
    "        self.input_label_indices_flat = tf.placeholder(tf.int32, [None], name=\"input_label_indices_flat\")\n",
    "        self.input_token_character_indices = tf.placeholder(tf.int32, [None, None], name=\"input_token_indices\")\n",
    "        self.input_token_lengths = tf.placeholder(tf.int32, [None], name=\"input_token_lengths\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Internal parameters\n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "        # Character-level LSTM\n",
    "        # Idea: reshape so that we have a tensor [number_of_token, max_token_length, token_embeddings_size], which we pass to the LSTM\n",
    "\n",
    "        # Character embedding layer\n",
    "        with tf.variable_scope(\"character_embedding\"):\n",
    "            self.character_embedding_weights = tf.get_variable(\"character_embedding_weights\",\n",
    "                    shape=[dataset.alphabet_size,character_embedding_dimension],initializer=initializer)\n",
    "            embedded_characters = tf.nn.embedding_lookup(self.character_embedding_weights,\n",
    "                                                             self.input_token_character_indices,\n",
    "                                                             name='embedded_characters')\n",
    "            if self.verbose: print(\"embedded_characters: {0}\".format(embedded_characters))\n",
    "#                 utils_tf.variable_summaries(self.character_embedding_weights)\n",
    "\n",
    "        # Character LSTM layer\n",
    "        with tf.variable_scope('character_lstm') as vs:\n",
    "            character_lstm_output = BLSTM(embedded_characters,\n",
    "                                                           character_lstm_hidden_state_dimension,\n",
    "                                                           initializer,\n",
    "                                                           sequence_length=self.input_token_lengths,\n",
    "                                                           output_sequence=False)\n",
    "            self.character_lstm_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=vs.name)\n",
    "\n",
    "        # Token embedding layer\n",
    "        with tf.variable_scope(\"token_embedding\"):\n",
    "            self.token_embedding_weights = tf.get_variable(\n",
    "                \"token_embedding_weights\",\n",
    "                shape=[dataset.vocabulary_size, token_embedding_dimension],\n",
    "                initializer=initializer,\n",
    "                trainable=not freeze_token_embeddings)\n",
    "            embedded_tokens = tf.nn.embedding_lookup(self.token_embedding_weights, self.input_token_indices)\n",
    "#             utils_tf.variable_summaries(self.token_embedding_weights)\n",
    "\n",
    "        # Concatenate character LSTM outputs and token embeddings\n",
    "        \n",
    "        with tf.variable_scope(\"concatenate_token_and_character_vectors\"):\n",
    "            token_lstm_input = tf.concat([character_lstm_output, embedded_tokens], axis=1, name='token_lstm_input')\n",
    "            if self.verbose: \n",
    "                print('embedded_tokens: {0}'.format(embedded_tokens))\n",
    "                print(\"token_lstm_input: {0}\".format(token_lstm_input))\n",
    "        \n",
    "\n",
    "        # Add dropout\n",
    "        with tf.variable_scope(\"dropout\"):\n",
    "            token_lstm_input_drop = tf.nn.dropout(token_lstm_input, self.dropout_keep_prob,\n",
    "                                                  name='token_lstm_input_drop')\n",
    "            if self.verbose: print(\"token_lstm_input_drop: {0}\".format(token_lstm_input_drop))\n",
    "            # https://www.tensorflow.org/api_guides/python/contrib.rnn\n",
    "            # Prepare data shape to match `rnn` function requirements\n",
    "            # Current data input shape: (batch_size, n_steps, n_input)\n",
    "            # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "            token_lstm_input_drop_expanded = tf.expand_dims(token_lstm_input_drop, axis=0,\n",
    "                                                            name='token_lstm_input_drop_expanded')\n",
    "            if self.verbose: print(\"token_lstm_input_drop_expanded: {0}\".format(token_lstm_input_drop_expanded))\n",
    "\n",
    "        # Token LSTM layer\n",
    "        with tf.variable_scope('token_lstm') as vs:\n",
    "            token_lstm_output = BLSTM(token_lstm_input_drop_expanded,\n",
    "                                                   token_lstm_hidden_state_dimension, initializer,\n",
    "                                                   output_sequence=True)\n",
    "            token_lstm_output_squeezed = tf.squeeze(token_lstm_output, axis=0)\n",
    "            self.token_lstm_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=vs.name)\n",
    "\n",
    "        # Needed only if Bidirectional LSTM is used for token level\n",
    "        with tf.variable_scope(\"feedforward_after_lstm\") as vs:\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[2 * token_lstm_hidden_state_dimension, token_lstm_hidden_state_dimension],\n",
    "                initializer=initializer)\n",
    "            b = tf.Variable(tf.constant(0.0, shape=[token_lstm_hidden_state_dimension]), name=\"bias\")\n",
    "            outputs = tf.nn.xw_plus_b(token_lstm_output_squeezed, W, b, name=\"output_before_tanh\")\n",
    "            outputs = tf.nn.tanh(outputs, name=\"output_after_tanh\")\n",
    "#             utils_tf.variable_summaries(W)\n",
    "#             utils_tf.variable_summaries(b)\n",
    "            self.token_lstm_variables += tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=vs.name)\n",
    "\n",
    "        with tf.variable_scope(\"feedforward_before_crf\") as vs:\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[token_lstm_hidden_state_dimension, dataset.number_of_classes],\n",
    "                initializer=initializer)\n",
    "            b = tf.Variable(tf.constant(0.0, shape=[dataset.number_of_classes]), name=\"bias\")\n",
    "            scores = tf.nn.xw_plus_b(outputs, W, b, name=\"scores\")\n",
    "            self.unary_scores = scores\n",
    "            self.predictions = tf.argmax(self.unary_scores, 1, name=\"predictions\")\n",
    "#             utils_tf.variable_summaries(W)\n",
    "#             utils_tf.variable_summaries(b)\n",
    "            self.feedforward_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=vs.name)\n",
    "\n",
    "        # CRF layer\n",
    "        with tf.variable_scope(\"crf\") as vs:\n",
    "            # Add start and end tokens\n",
    "            small_score = -1000.0\n",
    "            large_score = 0.0\n",
    "            sequence_length = tf.shape(self.unary_scores)[0]\n",
    "            unary_scores_with_start_and_end = tf.concat(\n",
    "                [self.unary_scores, tf.tile(tf.constant(small_score, shape=[1, 2]), [sequence_length, 1])], 1)\n",
    "            start_unary_scores = [[small_score] * dataset.number_of_classes + [large_score, small_score]]\n",
    "            end_unary_scores = [[small_score] * dataset.number_of_classes + [small_score, large_score]]\n",
    "            self.unary_scores = tf.concat([start_unary_scores, unary_scores_with_start_and_end, end_unary_scores],\n",
    "                                              0)\n",
    "            start_index = dataset.number_of_classes\n",
    "            end_index = dataset.number_of_classes + 1\n",
    "            input_label_indices_flat_with_start_and_end = tf.concat(\n",
    "                    [tf.constant(start_index, shape=[1]), self.input_label_indices_flat,\n",
    "                     tf.constant(end_index, shape=[1])], 0)\n",
    "\n",
    "            # Apply CRF layer\n",
    "            sequence_length = tf.shape(self.unary_scores)[0]\n",
    "            sequence_lengths = tf.expand_dims(sequence_length, axis=0, name='sequence_lengths')\n",
    "            unary_scores_expanded = tf.expand_dims(self.unary_scores, axis=0, name='unary_scores_expanded')\n",
    "            input_label_indices_flat_batch = tf.expand_dims(input_label_indices_flat_with_start_and_end, axis=0,\n",
    "                                                                name='input_label_indices_flat_batch')\n",
    "            if self.verbose: print('unary_scores_expanded: {0}'.format(unary_scores_expanded))\n",
    "            if self.verbose: print('input_label_indices_flat_batch: {0}'.format(input_label_indices_flat_batch))\n",
    "            if self.verbose: print(\"sequence_lengths: {0}\".format(sequence_lengths))\n",
    "            # https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/crf\n",
    "            # Compute the log-likelihood of the gold sequences and keep the transition params for inference at test time.\n",
    "            self.transition_parameters = tf.get_variable(\n",
    "                    \"transitions\",\n",
    "                    shape=[dataset.number_of_classes + 2, dataset.number_of_classes + 2],\n",
    "                    initializer=initializer)\n",
    "#                 utils_tf.variable_summaries(self.transition_parameters)\n",
    "            log_likelihood, _ = tf.contrib.crf.crf_log_likelihood(\n",
    "                    unary_scores_expanded, input_label_indices_flat_batch, sequence_lengths,\n",
    "                    transition_params=self.transition_parameters)\n",
    "            self.loss = tf.reduce_mean(-log_likelihood, name='cross_entropy_mean_loss')\n",
    "            self.accuracy = tf.constant(1)\n",
    "\n",
    "            self.crf_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=vs.name)\n",
    "\n",
    "\n",
    "        self.define_training_procedure(learning_rate=learning_rate,gradient_clipping_value=gradient_clipping_value,optimizer=optimizer)\n",
    "        self.summary_op = tf.summary.merge_all()\n",
    "        self.saver = tf.train.Saver(\n",
    "            max_to_keep=maximum_number_of_epochs)  # defaults to saving all variables\n",
    "    def define_training_procedure(self ,learning_rate ,gradient_clipping_value, optimizer='sgd'):\n",
    "        # Define training procedure\n",
    "        self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        if optimizer == 'adam':\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        elif optimizer == 'sgd':\n",
    "            self.optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        elif optimizer == 'adadelta':\n",
    "            self.optimizer = tf.train.AdadeltaOptimizer(learning_rate)\n",
    "        else:\n",
    "            raise ValueError('The lr_method parameter must be either adadelta, adam or sgd.')\n",
    "\n",
    "        grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
    "        if gradient_clipping_value:\n",
    "            grads_and_vars = [(tf.clip_by_value(grad, -gradient_clipping_value,\n",
    "                                                gradient_clipping_value), var)\n",
    "                              for grad, var in grads_and_vars]\n",
    "        # By defining a global_step variable and passing it to the optimizer we allow TensorFlow handle the counting of training steps for us.\n",
    "        # The global step will be automatically incremented by one every time you execute train_op.\n",
    "        self.train_op = self.optimizer.apply_gradients(grads_and_vars, global_step=self.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {'pretrained_model_folder':'../model',\n",
    "                      'dataset_text_folder':'../../../ML_EntityData/data/en',\n",
    "                      'character_embedding_dimension':25,\n",
    "                      'character_lstm_hidden_state_dimension':25,\n",
    "                      'check_for_digits_replaced_with_zeros':True,\n",
    "                      'check_for_lowercase':True,\n",
    "                      'debug':False,\n",
    "                      'dropout_rate':0.5,\n",
    "                      'experiment_name':'test',\n",
    "                      'freeze_token_embeddings':False,\n",
    "                      'gradient_clipping_value':5.0,\n",
    "                      'learning_rate':0.005,\n",
    "                      'load_only_pretrained_token_embeddings':False,\n",
    "                      'load_all_pretrained_token_embeddings':False,\n",
    "                      'main_evaluation_mode':'conll',\n",
    "                      'maximum_number_of_epochs':3,\n",
    "                      'number_of_cpu_threads':8,\n",
    "                      'number_of_gpus':0,\n",
    "                      'optimizer':'sgd',\n",
    "                      'output_folder':'../../../ML_EntityData/output',\n",
    "                      'patience':10,\n",
    "                      'plot_format':'pdf',\n",
    "                      'reload_character_embeddings':True,\n",
    "                      'reload_character_lstm':True,\n",
    "                      'reload_crf':True,\n",
    "                      'reload_feedforward':True,\n",
    "                      'reload_token_embeddings':True,\n",
    "                      'reload_token_lstm':True,\n",
    "                      'remap_unknown_tokens_to_unk':True,\n",
    "                      'spacylanguage':'en',\n",
    "                      'tagging_format':'bioes',\n",
    "                      'token_embedding_dimension':100,\n",
    "                      'token_lstm_hidden_state_dimension':100,\n",
    "                      'token_pretrained_embedding_filepath':'../../../ML_EntityData/embedding/glove.6B.100d.txt',\n",
    "                      'tokenizer':'spacy',\n",
    "                      'train_model':True,\n",
    "                      'use_character_lstm':True,\n",
    "                      'use_crf':True,\n",
    "                      'use_pretrained_model':False,\n",
    "                      'verbose':False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the validity of BRAT-formatted train set... Done.\n",
      "Checking compatibility between CONLL and BRAT for train_compatible_with_brat set ... Done.\n",
      "Checking validity of CONLL BIOES format... Done.\n",
      "Checking the validity of BRAT-formatted valid set... Done.\n",
      "Checking compatibility between CONLL and BRAT for valid_compatible_with_brat set ... Done.\n",
      "Checking validity of CONLL BIOES format... Done.\n",
      "Checking the validity of BRAT-formatted test set... Done.\n",
      "Checking compatibility between CONLL and BRAT for test_compatible_with_brat set ... Done.\n",
      "Checking validity of CONLL BIOES format... Done.\n",
      "Load dataset... done (44.11 seconds)\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import dataset as ds\n",
    "# Load dataset\n",
    "dataset_filepaths, dataset_brat_folders = utils.get_valid_dataset_filepaths(parameters)\n",
    "dataset = ds.Dataset(verbose=False, debug=False)\n",
    "token_to_vector = dataset.load_dataset(dataset_filepaths, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded_characters: Tensor(\"character_embedding/embedded_characters:0\", shape=(?, ?, 25), dtype=float32)\n",
      "embedded_tokens: Tensor(\"token_embedding/embedding_lookup:0\", shape=(?, 100), dtype=float32)\n",
      "token_lstm_input: Tensor(\"concatenate_token_and_character_vectors/token_lstm_input:0\", shape=(?, 150), dtype=float32)\n",
      "token_lstm_input_drop: Tensor(\"dropout/token_lstm_input_drop/mul:0\", shape=(?, 150), dtype=float32)\n",
      "token_lstm_input_drop_expanded: Tensor(\"dropout/token_lstm_input_drop_expanded:0\", shape=(1, ?, 150), dtype=float32)\n",
      "unary_scores_expanded: Tensor(\"crf/unary_scores_expanded:0\", shape=(1, ?, 19), dtype=float32)\n",
      "input_label_indices_flat_batch: Tensor(\"crf/input_label_indices_flat_batch:0\", shape=(1, ?), dtype=int32)\n",
      "sequence_lengths: Tensor(\"crf/sequence_lengths:0\", shape=(1,), dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\linhpn.VISC\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create model lstm+crf\n",
    "session_conf = tf.ConfigProto(\n",
    "            intra_op_parallelism_threads=parameters['number_of_cpu_threads'],\n",
    "            inter_op_parallelism_threads=parameters['number_of_cpu_threads'],\n",
    "            device_count={'CPU': 1, 'GPU': parameters['number_of_gpus']},\n",
    "            allow_soft_placement=True,\n",
    "            # automatically choose an existing and supported device to run the operations in case the specified one doesn't exist\n",
    "            log_device_placement=False\n",
    "        )\n",
    "sess = tf.Session(config=session_conf)\n",
    "\n",
    "with sess.as_default():\n",
    "    # Create model and initialize or load pretrained model\n",
    "    ### Instantiate the model\n",
    "    model = Char_BLSTM_CRF(dataset=dataset, token_embedding_dimension=parameters['token_embedding_dimension'],\n",
    "                       character_lstm_hidden_state_dimension=parameters['character_lstm_hidden_state_dimension'],\n",
    "                       token_lstm_hidden_state_dimension=parameters['token_lstm_hidden_state_dimension'],\n",
    "                       character_embedding_dimension=parameters['character_embedding_dimension'],\n",
    "                       gradient_clipping_value=parameters['gradient_clipping_value'],\n",
    "                       learning_rate=parameters['learning_rate'],\n",
    "                       freeze_token_embeddings=parameters['freeze_token_embeddings'],\n",
    "                       optimizer=parameters['optimizer'],\n",
    "                       maximum_number_of_epochs=parameters['maximum_number_of_epochs'])\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
