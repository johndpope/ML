{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\linhpn.VISC\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuroNER version: 1.0-dev\n",
      "TensorFlow version: 1.4.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import argparse\n",
    "from argparse import RawTextHelpFormatter\n",
    "import sys\n",
    "from ModelEntity import NeuroNER\n",
    "\n",
    "import warnings\n",
    "\n",
    "pretrained_model_folder='../model'\n",
    "dataset_text_folder='../data/en'\n",
    "character_embedding_dimension=25,\n",
    "character_lstm_hidden_state_dimension=25,\n",
    "check_for_digits_replaced_with_zeros=True,\n",
    "check_for_lowercase=True,\n",
    "debug=False,\n",
    "dropout_rate=0.5,\n",
    "experiment_name='experiment',\n",
    "freeze_token_embeddings=False,\n",
    "gradient_clipping_value=5.0,\n",
    "learning_rate=0.005,\n",
    "load_only_pretrained_token_embeddings=False,\n",
    "main_evaluation_mode='conll',\n",
    "maximum_number_of_epochs=10,\n",
    "number_of_cpu_threads=4,\n",
    "number_of_gpus=0,\n",
    "optimizer='sgd',\n",
    "output_folder='../output',\n",
    "patience=10,\n",
    "plot_format='pdf',\n",
    "reload_character_embeddings=True,\n",
    "reload_character_lstm=True,\n",
    "reload_crf=True,\n",
    "reload_feedforward=True,\n",
    "reload_token_embeddings=True,\n",
    "reload_token_lstm=True,\n",
    "remap_unknown_tokens_to_unk=True,\n",
    "spacylanguage='en',\n",
    "tagging_format='bioes',\n",
    "token_embedding_dimension=100,\n",
    "token_lstm_hidden_state_dimension=100,\n",
    "token_pretrained_embedding_filepath='../embedding/glove.6B.100d.txt',\n",
    "tokenizer='spacy',\n",
    "train_model=True,\n",
    "use_character_lstm=True,\n",
    "use_crf=True,\n",
    "use_pretrained_model=False,\n",
    "verbose=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'character_embedding_dimension': 25,\n",
      " 'character_lstm_hidden_state_dimension': 25,\n",
      " 'check_for_digits_replaced_with_zeros': 1,\n",
      " 'check_for_lowercase': 1,\n",
      " 'dataset_text_folder': '../data/en',\n",
      " 'debug': 0,\n",
      " 'dropout_rate': 0.5,\n",
      " 'experiment_name': 'test',\n",
      " 'freeze_token_embeddings': 0,\n",
      " 'gradient_clipping_value': 5.0,\n",
      " 'learning_rate': 0.005,\n",
      " 'load_all_pretrained_token_embeddings': 0,\n",
      " 'load_only_pretrained_token_embeddings': 0,\n",
      " 'main_evaluation_mode': 'conll',\n",
      " 'maximum_number_of_epochs': 3,\n",
      " 'number_of_cpu_threads': 8,\n",
      " 'number_of_gpus': 0,\n",
      " 'optimizer': 'sgd',\n",
      " 'output_folder': '../output',\n",
      " 'patience': 10,\n",
      " 'plot_format': 'pdf',\n",
      " 'pretrained_model_folder': '../model',\n",
      " 'reload_character_embeddings': 1,\n",
      " 'reload_character_lstm': 1,\n",
      " 'reload_crf': 1,\n",
      " 'reload_feedforward': 1,\n",
      " 'reload_token_embeddings': 1,\n",
      " 'reload_token_lstm': 1,\n",
      " 'remap_unknown_tokens_to_unk': 1,\n",
      " 'spacylanguage': 'en',\n",
      " 'tagging_format': 'bioes',\n",
      " 'token_embedding_dimension': 100,\n",
      " 'token_lstm_hidden_state_dimension': 100,\n",
      " 'token_pretrained_embedding_filepath': '../embedding/glove.6B.100d.txt',\n",
      " 'tokenizer': 'spacy',\n",
      " 'train_model': 1,\n",
      " 'use_character_lstm': 1,\n",
      " 'use_crf': 1,\n",
      " 'use_pretrained_model': 0,\n",
      " 'verbose': 0}\n",
      "Checking the validity of BRAT-formatted train set... Done.\n",
      "Checking compatibility between CONLL and BRAT for train_compatible_with_brat set ... Done.\n",
      "Checking validity of CONLL BIOES format... Done.\n",
      "Checking the validity of BRAT-formatted valid set... Done.\n",
      "Checking compatibility between CONLL and BRAT for valid_compatible_with_brat set ... Done.\n",
      "Checking validity of CONLL BIOES format... Done.\n",
      "Checking the validity of BRAT-formatted test set... Done.\n",
      "Checking compatibility between CONLL and BRAT for test_compatible_with_brat set ... Done.\n",
      "Checking validity of CONLL BIOES format... Done.\n",
      "Load dataset... done (42.26 seconds)\n",
      "Load token embeddings... done (0.15 seconds)\n",
      "number_of_token_original_case_found: 14618\n",
      "number_of_token_lowercase_found: 11723\n",
      "number_of_token_digits_replaced_with_zeros_found: 119\n",
      "number_of_token_lowercase_and_digits_replaced_with_zeros_found: 16\n",
      "number_of_loaded_word_vectors: 26476\n",
      "dataset.vocabulary_size: 28984\n"
     ]
    }
   ],
   "source": [
    "nn = NeuroNER(pretrained_model_folder='../model',dataset_text_folder='../data/en',maximum_number_of_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting epoch 0\n",
      "Training completed in 0.00 seconds\n",
      "Evaluate model on the train set\n",
      "Evaluate model on the valid set\n",
      "Evaluate model on the test set\n",
      "Generating plots for the train set\n",
      "Generating plots for the valid set\n",
      "Generating plots for the test set\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'f1_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b56621912126>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\linhpn.VISC\\Documents\\PyCharm Project\\NameEntityRecognition\\src\\ModelEntity.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    433\u001b[0m                 \u001b[1;31m# Evaluate model: save and plot results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m                 evaluate.evaluate_model(results, dataset, y_pred, y_true, stats_graph_folder, epoch_number,\n\u001b[1;32m--> 435\u001b[1;33m                                         epoch_start_time, output_filepaths, parameters)\n\u001b[0m\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'use_pretrained_model'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train_model'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\linhpn.VISC\\Documents\\PyCharm Project\\NameEntityRecognition\\src\\evaluate.py\u001b[0m in \u001b[0;36mevaluate_model\u001b[1;34m(results, dataset, y_pred_all, y_true_all, stats_graph_folder, epoch_number, epoch_start_time, output_filepaths, parameters, verbose)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m     \u001b[1;32mif\u001b[0m  \u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train_model'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m'train'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutput_filepaths\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m'valid'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutput_filepaths\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m         \u001b[0mplot_f1_vs_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstats_graph_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'f1_score'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m         \u001b[0mplot_f1_vs_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstats_graph_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'accuracy_score'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[0mplot_f1_vs_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstats_graph_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'f1_conll'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\linhpn.VISC\\Documents\\PyCharm Project\\NameEntityRecognition\\src\\evaluate.py\u001b[0m in \u001b[0;36mplot_f1_vs_epoch\u001b[1;34m(results, stats_graph_folder, metric, parameters, from_json)\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[0mresult_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'epoch'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meidx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m    \u001b[1;31m# when loading json file\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdataset_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset_types\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m             \u001b[0mf1_dict_all\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset_type\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_epoch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset_type\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'f1_score'"
     ]
    }
   ],
   "source": [
    "nn.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "import train\n",
    "import dataset as ds\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "from lstm_crf import EntityLSTM\n",
    "import utils\n",
    "import os\n",
    "import conll2brat\n",
    "import glob\n",
    "import codecs\n",
    "import shutil\n",
    "import time\n",
    "import copy\n",
    "import evaluate\n",
    "import random\n",
    "import pickle\n",
    "import brat2conll\n",
    "import numpy as np\n",
    "import utils_nlp\n",
    "import distutils.util as distutils_util\n",
    "import configparser\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {'pretrained_model_folder':'../model',\n",
    "                      'dataset_text_folder':'../data/en',\n",
    "                      'character_embedding_dimension':25,\n",
    "                      'character_lstm_hidden_state_dimension':25,\n",
    "                      'check_for_digits_replaced_with_zeros':True,\n",
    "                      'check_for_lowercase':True,\n",
    "                      'debug':False,\n",
    "                      'dropout_rate':0.5,\n",
    "                      'experiment_name':'test',\n",
    "                      'freeze_token_embeddings':False,\n",
    "                      'gradient_clipping_value':5.0,\n",
    "                      'learning_rate':0.005,\n",
    "                      'load_only_pretrained_token_embeddings':False,\n",
    "                      'load_all_pretrained_token_embeddings':False,\n",
    "                      'main_evaluation_mode':'conll',\n",
    "                      'maximum_number_of_epochs':100,\n",
    "                      'number_of_cpu_threads':8,\n",
    "                      'number_of_gpus':0,\n",
    "                      'optimizer':'sgd',\n",
    "                      'output_folder':'../output',\n",
    "                      'patience':10,\n",
    "                      'plot_format':'pdf',\n",
    "                      'reload_character_embeddings':True,\n",
    "                      'reload_character_lstm':True,\n",
    "                      'reload_crf':True,\n",
    "                      'reload_feedforward':True,\n",
    "                      'reload_token_embeddings':True,\n",
    "                      'reload_token_lstm':True,\n",
    "                      'remap_unknown_tokens_to_unk':True,\n",
    "                      'spacylanguage':'en',\n",
    "                      'tagging_format':'bioes',\n",
    "                      'token_embedding_dimension':100,\n",
    "                      'token_lstm_hidden_state_dimension':100,\n",
    "                      'token_pretrained_embedding_filepath':'../embedding/glove.6B.100d.txt',\n",
    "                      'tokenizer':'spacy',\n",
    "                      'train_model':True,\n",
    "                      'use_character_lstm':True,\n",
    "                      'use_crf':True,\n",
    "                      'use_pretrained_model':False,\n",
    "                      'verbose':False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "argument_default_value = 'argument_default_dummy_value_please_ignore_d41d8cd98f00b204e9800998ecf8427e'\n",
    "prediction_count = 0\n",
    "\n",
    "def _create_stats_graph_folder(parameters):\n",
    "        # Initialize stats_graph_folder\n",
    "        experiment_timestamp = utils.get_current_time_in_miliseconds()\n",
    "        dataset_name = utils.get_basename_without_extension(parameters['dataset_text_folder'])\n",
    "        model_name = '{0}_{1}'.format(dataset_name, experiment_timestamp)\n",
    "        utils.create_folder_if_not_exists(parameters['output_folder'])\n",
    "        stats_graph_folder = os.path.join(parameters['output_folder'], model_name)  # Folder where to save graphs\n",
    "        utils.create_folder_if_not_exists(stats_graph_folder)\n",
    "        return stats_graph_folder, experiment_timestamp\n",
    "\n",
    "def _load_parameters(parameters_filepath, arguments={}, verbose=True):\n",
    "        '''\n",
    "        Load parameters from the ini file if specified, take into account any command line argument, and ensure that each parameter is cast to the correct type.\n",
    "        Command line arguments take precedence over parameters specified in the parameter file.\n",
    "        '''\n",
    "        parameters = {'pretrained_model_folder':'../model',\n",
    "                      'dataset_text_folder':'../data/en',\n",
    "                      'character_embedding_dimension':25,\n",
    "                      'character_lstm_hidden_state_dimension':25,\n",
    "                      'check_for_digits_replaced_with_zeros':True,\n",
    "                      'check_for_lowercase':True,\n",
    "                      'debug':False,\n",
    "                      'dropout_rate':0.5,\n",
    "                      'experiment_name':'test',\n",
    "                      'freeze_token_embeddings':False,\n",
    "                      'gradient_clipping_value':5.0,\n",
    "                      'learning_rate':0.005,\n",
    "                      'load_only_pretrained_token_embeddings':False,\n",
    "                      'load_all_pretrained_token_embeddings':False,\n",
    "                      'main_evaluation_mode':'conll',\n",
    "                      'maximum_number_of_epochs':100,\n",
    "                      'number_of_cpu_threads':8,\n",
    "                      'number_of_gpus':0,\n",
    "                      'optimizer':'sgd',\n",
    "                      'output_folder':'../output',\n",
    "                      'patience':10,\n",
    "                      'plot_format':'pdf',\n",
    "                      'reload_character_embeddings':True,\n",
    "                      'reload_character_lstm':True,\n",
    "                      'reload_crf':True,\n",
    "                      'reload_feedforward':True,\n",
    "                      'reload_token_embeddings':True,\n",
    "                      'reload_token_lstm':True,\n",
    "                      'remap_unknown_tokens_to_unk':True,\n",
    "                      'spacylanguage':'en',\n",
    "                      'tagging_format':'bioes',\n",
    "                      'token_embedding_dimension':100,\n",
    "                      'token_lstm_hidden_state_dimension':100,\n",
    "                      'token_pretrained_embedding_filepath':'../embedding/glove.6B.100d.txt',\n",
    "                      'tokenizer':'spacy',\n",
    "                      'train_model':True,\n",
    "                      'use_character_lstm':True,\n",
    "                      'use_crf':True,\n",
    "                      'use_pretrained_model':False,\n",
    "                      'verbose':False}\n",
    "        # If a parameter file is specified, load it\n",
    "        if len(parameters_filepath) > 0:\n",
    "            conf_parameters = configparser.ConfigParser()\n",
    "            conf_parameters.read(parameters_filepath)\n",
    "            nested_parameters = utils.convert_configparser_to_dictionary(conf_parameters)\n",
    "            for k,v in nested_parameters.items():\n",
    "                parameters.update(v)\n",
    "        # Ensure that any arguments the specified in the command line overwrite parameters specified in the parameter file\n",
    "        for k,v in arguments.items():\n",
    "            if arguments[k] != arguments['argument_default_value']:\n",
    "                parameters[k] = v\n",
    "        for k,v in parameters.items():\n",
    "            v = str(v)\n",
    "            # If the value is a list delimited with a comma, choose one element at random.\n",
    "            if ',' in v:\n",
    "                v = random.choice(v.split(','))\n",
    "                parameters[k] = v\n",
    "            # Ensure that each parameter is cast to the correct type\n",
    "            if k in ['character_embedding_dimension','character_lstm_hidden_state_dimension','token_embedding_dimension',\n",
    "                     'token_lstm_hidden_state_dimension','patience','maximum_number_of_epochs','maximum_training_time','number_of_cpu_threads','number_of_gpus']:\n",
    "                parameters[k] = int(v)\n",
    "            elif k in ['dropout_rate', 'learning_rate', 'gradient_clipping_value']:\n",
    "                parameters[k] = float(v)\n",
    "            elif k in ['remap_unknown_tokens_to_unk', 'use_character_lstm', 'use_crf', 'train_model', 'use_pretrained_model', 'debug', 'verbose',\n",
    "                     'reload_character_embeddings', 'reload_character_lstm', 'reload_token_embeddings', 'reload_token_lstm', 'reload_feedforward', 'reload_crf',\n",
    "                     'check_for_lowercase', 'check_for_digits_replaced_with_zeros', 'freeze_token_embeddings', 'load_only_pretrained_token_embeddings', 'load_all_pretrained_token_embeddings']:\n",
    "                parameters[k] = distutils_util.strtobool(v)\n",
    "        # If loading pretrained model, set the model hyperparameters according to the pretraining parameters\n",
    "        if parameters['use_pretrained_model']:\n",
    "            pretraining_parameters = self._load_parameters(parameters_filepath=os.path.join(parameters['pretrained_model_folder'], 'parameters.ini'), verbose=False)[0]\n",
    "            for name in ['use_character_lstm', 'character_embedding_dimension', 'character_lstm_hidden_state_dimension', 'token_embedding_dimension', 'token_lstm_hidden_state_dimension', 'use_crf']:\n",
    "                if parameters[name] != pretraining_parameters[name]:\n",
    "                    print('WARNING: parameter {0} was overwritten from {1} to {2} to be consistent with the pretrained model'.format(name, parameters[name], pretraining_parameters[name]))\n",
    "                    parameters[name] = pretraining_parameters[name]\n",
    "        if verbose: pprint(parameters)\n",
    "        # Update conf_parameters to reflect final parameter values\n",
    "        conf_parameters = configparser.ConfigParser()\n",
    "        conf_parameters.read(os.path.join('test', 'test-parameters-training.ini'))\n",
    "        parameter_to_section = utils.get_parameter_to_section_of_configparser(conf_parameters)\n",
    "        for k, v in parameters.items():\n",
    "            conf_parameters.set(parameter_to_section[k], k, str(v))\n",
    "\n",
    "        return parameters, conf_parameters\n",
    "\n",
    "def _get_valid_dataset_filepaths(parameters, dataset_types=['train', 'valid', 'test', 'deploy']):\n",
    "        dataset_filepaths = {}\n",
    "        dataset_brat_folders = {}\n",
    "        for dataset_type in dataset_types:\n",
    "            dataset_filepaths[dataset_type] = os.path.join(parameters['dataset_text_folder'],\n",
    "                                                           '{0}.txt'.format(dataset_type))\n",
    "            dataset_brat_folders[dataset_type] = os.path.join(parameters['dataset_text_folder'], dataset_type)\n",
    "            dataset_compatible_with_brat_filepath = os.path.join(parameters['dataset_text_folder'],\n",
    "                                                                 '{0}_compatible_with_brat.txt'.format(dataset_type))\n",
    "\n",
    "            # Conll file exists\n",
    "            if os.path.isfile(dataset_filepaths[dataset_type]) and os.path.getsize(dataset_filepaths[dataset_type]) > 0:\n",
    "                # Brat text files exist\n",
    "                if os.path.exists(dataset_brat_folders[dataset_type]) and len(\n",
    "                        glob.glob(os.path.join(dataset_brat_folders[dataset_type], '*.txt'))) > 0:\n",
    "\n",
    "                    # Check compatibility between conll and brat files\n",
    "                    brat2conll.check_brat_annotation_and_text_compatibility(dataset_brat_folders[dataset_type])\n",
    "                    if os.path.exists(dataset_compatible_with_brat_filepath):\n",
    "                        dataset_filepaths[dataset_type] = dataset_compatible_with_brat_filepath\n",
    "                    conll2brat.check_compatibility_between_conll_and_brat_text(dataset_filepaths[dataset_type],\n",
    "                                                                                  dataset_brat_folders[dataset_type])\n",
    "\n",
    "                # Brat text files do not exist\n",
    "                else:\n",
    "\n",
    "                    # Populate brat text and annotation files based on conll file\n",
    "                    conll2brat.conll_to_brat(dataset_filepaths[dataset_type], dataset_compatible_with_brat_filepath,\n",
    "                                                dataset_brat_folders[dataset_type], dataset_brat_folders[dataset_type])\n",
    "                    dataset_filepaths[dataset_type] = dataset_compatible_with_brat_filepath\n",
    "\n",
    "            # Conll file does not exist\n",
    "            else:\n",
    "                # Brat text files exist\n",
    "                if os.path.exists(dataset_brat_folders[dataset_type]) and len(\n",
    "                        glob.glob(os.path.join(dataset_brat_folders[dataset_type], '*.txt'))) > 0:\n",
    "                    dataset_filepath_for_tokenizer = os.path.join(parameters['dataset_text_folder'],\n",
    "                                                                  '{0}_{1}.txt'.format(dataset_type,\n",
    "                                                                                       parameters['tokenizer']))\n",
    "                    if os.path.exists(dataset_filepath_for_tokenizer):\n",
    "                        conll2brat.check_compatibility_between_conll_and_brat_text(dataset_filepath_for_tokenizer,\n",
    "                                                                                      dataset_brat_folders[\n",
    "                                                                                          dataset_type])\n",
    "                    else:\n",
    "                        # Populate conll file based on brat files\n",
    "                        brat2conll.brat_to_conll(dataset_brat_folders[dataset_type], dataset_filepath_for_tokenizer,\n",
    "                                                    parameters['tokenizer'], parameters['spacylanguage'])\n",
    "                    dataset_filepaths[dataset_type] = dataset_filepath_for_tokenizer\n",
    "\n",
    "                # Brat text files do not exist\n",
    "                else:\n",
    "                    del dataset_filepaths[dataset_type]\n",
    "                    del dataset_brat_folders[dataset_type]\n",
    "                    continue\n",
    "\n",
    "            if parameters['tagging_format'] == 'bioes':\n",
    "                # Generate conll file with BIOES format\n",
    "                bioes_filepath = os.path.join(parameters['dataset_text_folder'], '{0}_bioes.txt'.format(\n",
    "                    utils.get_basename_without_extension(dataset_filepaths[dataset_type])))\n",
    "                utils_nlp.convert_conll_from_bio_to_bioes(dataset_filepaths[dataset_type], bioes_filepath)\n",
    "                dataset_filepaths[dataset_type] = bioes_filepath\n",
    "\n",
    "        return dataset_filepaths, dataset_brat_folders\n",
    "\n",
    "def _check_parameter_compatiblity(parameters, dataset_filepaths):\n",
    "        # Check mode of operation\n",
    "        if parameters['train_model']:\n",
    "            if 'train' not in dataset_filepaths or 'valid' not in dataset_filepaths:\n",
    "                raise IOError(\n",
    "                    \"If train_model is set to True, both train and valid set must exist in the specified dataset folder: {0}\".format(\n",
    "                        parameters['dataset_text_folder']))\n",
    "        elif parameters['use_pretrained_model']:\n",
    "            if 'train' in dataset_filepaths and 'valid' in dataset_filepaths:\n",
    "                print(\n",
    "                    \"WARNING: train and valid set exist in the specified dataset folder, but train_model is set to FALSE: {0}\".format(\n",
    "                        parameters['dataset_text_folder']))\n",
    "            if 'test' not in dataset_filepaths and 'deploy' not in dataset_filepaths:\n",
    "                raise IOError(\n",
    "                    \"For prediction mode, either test set and deploy set must exist in the specified dataset folder: {0}\".format(\n",
    "                        parameters['dataset_text_folder']))\n",
    "        else:  # if not parameters['train_model'] and not parameters['use_pretrained_model']:\n",
    "            raise ValueError('At least one of train_model and use_pretrained_model must be set to True.')\n",
    "\n",
    "        if parameters['use_pretrained_model']:\n",
    "            if all([not parameters[s] for s in\n",
    "                    ['reload_character_embeddings', 'reload_character_lstm', 'reload_token_embeddings',\n",
    "                     'reload_token_lstm', 'reload_feedforward', 'reload_crf']]):\n",
    "                raise ValueError(\n",
    "                    'If use_pretrained_model is set to True, at least one of reload_character_embeddings, reload_character_lstm, reload_token_embeddings, reload_token_lstm, reload_feedforward, reload_crf must be set to True.')\n",
    "\n",
    "        if parameters['gradient_clipping_value'] < 0:\n",
    "            parameters['gradient_clipping_value'] = abs(parameters['gradient_clipping_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the validity of BRAT-formatted train set... Done.\n",
      "Checking compatibility between CONLL and BRAT for train_compatible_with_brat set ... Done.\n",
      "Checking validity of CONLL BIOES format... Done.\n",
      "Checking the validity of BRAT-formatted valid set... Done.\n",
      "Checking compatibility between CONLL and BRAT for valid_compatible_with_brat set ... Done.\n",
      "Checking validity of CONLL BIOES format... Done.\n",
      "Checking the validity of BRAT-formatted test set... Done.\n",
      "Checking compatibility between CONLL and BRAT for test_compatible_with_brat set ... Done.\n",
      "Checking validity of CONLL BIOES format... Done.\n",
      "Load dataset... done (46.21 seconds)\n"
     ]
    }
   ],
   "source": [
    "dataset_filepaths, dataset_brat_folders = _get_valid_dataset_filepaths(parameters)\n",
    "dataset = ds.Dataset(verbose=False, debug=False)\n",
    "token_to_vector = dataset.load_dataset(dataset_filepaths, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session_conf = tf.ConfigProto(\n",
    "            intra_op_parallelism_threads=parameters['number_of_cpu_threads'],\n",
    "            inter_op_parallelism_threads=parameters['number_of_cpu_threads'],\n",
    "            device_count={'CPU': 1, 'GPU': parameters['number_of_gpus']},\n",
    "            allow_soft_placement=True,\n",
    "            # automatically choose an existing and supported device to run the operations in case the specified one doesn't exist\n",
    "            log_device_placement=False\n",
    "        )\n",
    "sess = tf.Session(config=session_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sess.as_default():\n",
    "            # Create model and initialize or load pretrained model\n",
    "            ### Instantiate the model\n",
    "            model = EntityLSTM(dataset=dataset, token_embedding_dimension = parameters['token_embedding_dimension'],character_lstm_hidden_state_dimension=parameters['character_lstm_hidden_state_dimension'],\n",
    "                               token_lstm_hidden_state_dimension=parameters['token_lstm_hidden_state_dimension'],character_embedding_dimension=parameters['character_embedding_dimension'],\n",
    "                               use_crf=parameters['use_crf'],\n",
    "                               use_character_lstm=parameters['use_character_lstm'],\n",
    "                               gradient_clipping_value=parameters['gradient_clipping_value'],\n",
    "                               learning_rate=parameters['learning_rate'],\n",
    "                               freeze_token_embeddings=parameters['freeze_token_embeddings'],\n",
    "                               optimizer=parameters['optimizer'],maximum_number_of_epochs=parameters['maximum_number_of_epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../model\n"
     ]
    }
   ],
   "source": [
    "print(parameters['pretrained_model_folder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../model\\model.ckpt\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "transition_params_trained = model.restore_from_pretrained_model(dataset, sess , model_pathfile=os.path.join(parameters['pretrained_model_folder'],'model.ckpt'),\n",
    "                                                                                     dataset_pathfile=(parameters['pretrained_model_folder']+'/dataset.pickle'),\n",
    "                                                                                     embedding_filepath= parameters['token_pretrained_embedding_filepath'],\n",
    "                                                                                     character_dimension = parameters['character_embedding_dimension'],\n",
    "                                                                                     token_dimension=parameters['token_embedding_dimension'],token_to_vector=token_to_vector)\n",
    "del token_to_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load token embeddings... done (0.18 seconds)\n",
      "number_of_token_original_case_found: 14618\n",
      "number_of_token_lowercase_found: 11723\n",
      "number_of_token_digits_replaced_with_zeros_found: 119\n",
      "number_of_token_lowercase_and_digits_replaced_with_zeros_found: 16\n",
      "number_of_loaded_word_vectors: 26476\n",
      "dataset.vocabulary_size: 28984\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "if not parameters['use_pretrained_model']:\n",
    "                model.load_pretrained_token_embeddings(sess, dataset,embedding_filepath=parameters['token_pretrained_embedding_filepath'],\n",
    "                                                       check_lowercase= parameters['check_for_lowercase'],check_digits=parameters['check_for_digits_replaced_with_zeros'],\n",
    "                                                       token_to_vector=token_to_vector)\n",
    "                transition_params_trained = np.random.rand(len(dataset.unique_labels) + 2,\n",
    "                                                                len(dataset.unique_labels) + 2)\n",
    "else:\n",
    "                transition_params_trained = model.restore_from_pretrained_model(dataset, sess , model_pathfile=os.path.join(parameters['pretrained_model_folder'],'model.ckpt'),\n",
    "                                                                                     dataset_pathfile=os.path.join(parameters['pretrained_model_folder'],'/dataset.pickle'),\n",
    "                                                                                     embedding_filepath= parameters['token_pretrained_embedding_filepath'],\n",
    "                                                                                     character_dimension = parameters['character_dimension'],\n",
    "                                                                                     token_dimension=parameters['token_dimension'],token_to_vector=token_to_vector)\n",
    "del token_to_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_graph_folder, experiment_timestamp = _create_stats_graph_folder(parameters)\n",
    "\n",
    "        # Initialize and save execution details\n",
    "start_time = time.time()\n",
    "results = {}\n",
    "results['epoch'] = {}\n",
    "results['execution_details'] = {}\n",
    "results['execution_details']['train_start'] = start_time\n",
    "results['execution_details']['time_stamp'] = experiment_timestamp\n",
    "results['execution_details']['early_stop'] = False\n",
    "results['execution_details']['keyboard_interrupt'] = False\n",
    "results['execution_details']['num_epochs'] = 0\n",
    "results['model_options'] = copy.copy(parameters)\n",
    "\n",
    "model_folder = os.path.join(stats_graph_folder, 'model')\n",
    "utils.create_folder_if_not_exists(model_folder)\n",
    "# with open(os.path.join(model_folder, 'parameters.ini'), 'w') as parameters_file:\n",
    "#     conf_parameters.write(parameters_file)\n",
    "pickle.dump(dataset, open(os.path.join(model_folder, 'dataset.pickle'), 'wb'))\n",
    "\n",
    "tensorboard_log_folder = os.path.join(stats_graph_folder, 'tensorboard_logs')\n",
    "utils.create_folder_if_not_exists(tensorboard_log_folder)\n",
    "tensorboard_log_folders = {}\n",
    "for dataset_type in dataset_filepaths.keys():\n",
    "    tensorboard_log_folders[dataset_type] = os.path.join(stats_graph_folder, 'tensorboard_logs', dataset_type)\n",
    "    utils.create_folder_if_not_exists(tensorboard_log_folders[dataset_type])\n",
    "\n",
    "        # Instantiate the writers for TensorBoard\n",
    "writers = {}\n",
    "for dataset_type in dataset_filepaths.keys():\n",
    "    writers[dataset_type] = tf.summary.FileWriter(tensorboard_log_folders[dataset_type], graph=sess.graph)\n",
    "embedding_writer = tf.summary.FileWriter(\n",
    "            model_folder)  # embedding_writer has to write in model_folder, otherwise TensorBoard won't be able to view embeddings\n",
    "\n",
    "embeddings_projector_config = projector.ProjectorConfig()\n",
    "tensorboard_token_embeddings = embeddings_projector_config.embeddings.add()\n",
    "tensorboard_token_embeddings.tensor_name = model.token_embedding_weights.name\n",
    "token_list_file_path = os.path.join(model_folder, 'tensorboard_metadata_tokens.tsv')\n",
    "tensorboard_token_embeddings.metadata_path = os.path.relpath(token_list_file_path, '..')\n",
    "\n",
    "tensorboard_character_embeddings = embeddings_projector_config.embeddings.add()\n",
    "tensorboard_character_embeddings.tensor_name = model.character_embedding_weights.name\n",
    "character_list_file_path = os.path.join(model_folder, 'tensorboard_metadata_characters.tsv')\n",
    "tensorboard_character_embeddings.metadata_path = os.path.relpath(character_list_file_path, '..')\n",
    "\n",
    "projector.visualize_embeddings(embedding_writer, embeddings_projector_config)\n",
    "\n",
    "        # Write metadata for TensorBoard embeddings\n",
    "token_list_file = codecs.open(token_list_file_path, 'w', 'UTF-8')\n",
    "for token_index in range(dataset.vocabulary_size):\n",
    "    token_list_file.write('{0}\\n'.format(dataset.index_to_token[token_index]))\n",
    "token_list_file.close()\n",
    "\n",
    "character_list_file = codecs.open(character_list_file_path, 'w', 'UTF-8')\n",
    "for character_index in range(dataset.alphabet_size):\n",
    "    if character_index == dataset.PADDING_CHARACTER_INDEX:\n",
    "           character_list_file.write('PADDING\\n')\n",
    "    else:\n",
    "        character_list_file.write('{0}\\n'.format(dataset.index_to_character[character_index]))\n",
    "character_list_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting epoch 0\n",
      "Training completed in 0.00 seconds\n",
      "Evaluate model on the train set\n",
      "\n",
      "Evaluate model on the valid set\n",
      "\n",
      "Evaluate model on the test set\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-a1c7718d1434>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m                 \u001b[1;31m# Early stop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m                 \u001b[0mvalid_f1_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'epoch'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mepoch_number\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'valid'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'f1_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'micro'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mvalid_f1_score\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mprevious_best_valid_f1_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                     \u001b[0mbad_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "bad_counter = 0  # number of epochs with no improvement on the validation test in terms of F1-score\n",
    "previous_best_valid_f1_score = 0\n",
    "epoch_number = -1\n",
    "\n",
    "while True:\n",
    "                step = 0\n",
    "                epoch_number += 1\n",
    "                print('\\nStarting epoch {0}'.format(epoch_number))\n",
    "\n",
    "                epoch_start_time = time.time()\n",
    "\n",
    "                if epoch_number != 0:\n",
    "                    # Train model: loop over all sequences of training set with shuffling\n",
    "                    sequence_numbers = list(range(len(dataset.token_indices['train'])))\n",
    "                    random.shuffle(sequence_numbers)\n",
    "                    for sequence_number in sequence_numbers:\n",
    "                        transition_params_trained = train.train_step(sess, dataset, sequence_number, model, parameters)\n",
    "                        step += 1\n",
    "                        if step % 10 == 0:\n",
    "                            print('Training {0:.2f}% done'.format(step / len(sequence_numbers) * 100), end='\\r',\n",
    "                                  flush=True)\n",
    "\n",
    "                epoch_elapsed_training_time = time.time() - epoch_start_time\n",
    "                print('Training completed in {0:.2f} seconds'.format(epoch_elapsed_training_time), flush=True)\n",
    "\n",
    "                y_pred, y_true, output_filepaths = train.predict_labels(sess=sess,model= model,transition_params_trained= transition_params_trained,\n",
    "                                                                        dataset=dataset,epoch_number= epoch_number,\n",
    "                                                                       stats_graph_folder= stats_graph_folder,dataset_filepaths= dataset_filepaths,\n",
    "                                                                       tagging_format= parameters['tagging_format'], main_evaluation_mode=parameters['main_evaluation_mode'])\n",
    "\n",
    "#                 # Evaluate model: save and plot results\n",
    "#                 evaluate.evaluate_model(results, dataset, y_pred, y_true, stats_graph_folder, epoch_number,\n",
    "#                                         epoch_start_time, output_filepaths, parameters)\n",
    "\n",
    "                if parameters['use_pretrained_model'] and not parameters['train_model']:\n",
    "                    conll2brat.output_brat(output_filepaths, dataset_brat_folders, stats_graph_folder)\n",
    "                    break\n",
    "\n",
    "                # Save model\n",
    "                model.saver.save(sess, os.path.join(model_folder, 'model_{0:05d}.ckpt'.format(epoch_number)))\n",
    "\n",
    "                # Save TensorBoard logs\n",
    "                summary = sess.run(model.summary_op, feed_dict=None)\n",
    "                writers['train'].add_summary(summary, epoch_number)\n",
    "                writers['train'].flush()\n",
    "                utils.copytree(writers['train'].get_logdir(), model_folder)\n",
    "\n",
    "                # Early stop\n",
    "                valid_f1_score = results['epoch'][epoch_number][0]['valid']['f1_score']['micro']\n",
    "                if valid_f1_score > previous_best_valid_f1_score:\n",
    "                    bad_counter = 0\n",
    "                    previous_best_valid_f1_score = valid_f1_score\n",
    "                    conll2brat.output_brat(output_filepaths, dataset_brat_folders, stats_graph_folder,\n",
    "                                              overwrite=True)\n",
    "                    self.transition_params_trained = transition_params_trained\n",
    "                else:\n",
    "                    bad_counter += 1\n",
    "                print(\"The last {0} epochs have not shown improvements on the validation set.\".format(bad_counter))\n",
    "\n",
    "                if bad_counter >= parameters['patience']:\n",
    "                    print('Early Stop!')\n",
    "                    results['execution_details']['early_stop'] = True\n",
    "                    break\n",
    "\n",
    "                if epoch_number >= parameters['maximum_number_of_epochs']: break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction_count=0\n",
    "def predict( text):\n",
    "        \n",
    "\n",
    "#         if prediction_count == 1:\n",
    "        parameters['dataset_text_folder'] = os.path.join('..', 'data', 'temp')\n",
    "        stats_graph_folder, _ = _create_stats_graph_folder(parameters)\n",
    "\n",
    "        # Update the deploy folder, file, and dataset\n",
    "        dataset_type = 'deploy'\n",
    "        ### Delete all deployment data\n",
    "        for filepath in glob.glob(os.path.join(parameters['dataset_text_folder'], '{0}*'.format(dataset_type))):\n",
    "            if os.path.isdir(filepath):\n",
    "                shutil.rmtree(filepath)\n",
    "            else:\n",
    "                os.remove(filepath)\n",
    "        ### Create brat folder and file\n",
    "        dataset_brat_deploy_folder = os.path.join(parameters['dataset_text_folder'], dataset_type)\n",
    "        utils.create_folder_if_not_exists(dataset_brat_deploy_folder)\n",
    "        dataset_brat_deploy_filepath = os.path.join(dataset_brat_deploy_folder, 'temp_{0}.txt'.format(\n",
    "            str(prediction_count).zfill(5)))  # self._get_dataset_brat_deploy_filepath(dataset_brat_deploy_folder)\n",
    "        with codecs.open(dataset_brat_deploy_filepath, 'w', 'UTF-8') as f:\n",
    "            f.write(text)\n",
    "        ### Update deploy filepaths\n",
    "        dataset_filepaths, dataset_brat_folders = _get_valid_dataset_filepaths(parameters,\n",
    "                                                                                    dataset_types=[dataset_type])\n",
    "        dataset_filepaths.update(dataset_filepaths)\n",
    "        dataset_brat_folders.update(dataset_brat_folders)\n",
    "        ### Update the dataset for the new deploy set\n",
    "        dataset.update_dataset(dataset_filepaths, [dataset_type])\n",
    "\n",
    "        # Predict labels and output brat\n",
    "        output_filepaths = {}\n",
    "        prediction_output = train.prediction_step(sess, dataset, dataset_type, model,\n",
    "                                                  transition_params_trained, stats_graph_folder,\n",
    "                                                  prediction_count, dataset_filepaths,parameters['tagging_format'],\n",
    "                                                 parameters['main_evaluation_mode'])\n",
    "        _, _, output_filepaths[dataset_type] = prediction_output\n",
    "        conll2brat.output_brat(output_filepaths, dataset_brat_folders, stats_graph_folder, overwrite=True)\n",
    "\n",
    "        # Print and output result\n",
    "        text_filepath = os.path.join(stats_graph_folder, 'brat', 'deploy',\n",
    "                                     os.path.basename(dataset_brat_deploy_filepath))\n",
    "        annotation_filepath = os.path.join(stats_graph_folder, 'brat', 'deploy', '{0}.ann'.format(\n",
    "            utils.get_basename_without_extension(dataset_brat_deploy_filepath)))\n",
    "        text2, entities = brat2conll.get_entities_from_brat(text_filepath, annotation_filepath, verbose=True)\n",
    "        assert (text == text2)\n",
    "        return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting deploy set from BRAT to CONLL... Done.\n",
      "Converting CONLL from BIO to BIOES format... Done.\n",
      "Predict labels for the deploy set\n",
      "Formatting 000_deploy set from CONLL to BRAT... Done.\n",
      "\n",
      "text:\n",
      "my name is Ngoc Linh\n",
      "\n",
      "entity: {'id': 'T1', 'type': 'PER', 'start': 11, 'end': 20, 'text': 'Ngoc Linh'}\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'end': 20, 'id': 'T1', 'start': 11, 'text': 'Ngoc Linh', 'type': 'PER'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('my name is Ngoc Linh')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
